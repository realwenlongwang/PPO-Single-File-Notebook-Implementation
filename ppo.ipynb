{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tnrange\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wandb\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "class PPOBuffer():\n",
    "    def __init__(self, observation_dim, action_dim, capacity, gamma, lam):\n",
    "        self.obs_buf = np.zeros(combined_shape(capacity, observation_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(capacity, action_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.rtg_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        # self.logp_buf = np.zeros(combined_shape(capacity, action_dim), dtype=np.float32)\n",
    "        self.capacity = capacity\n",
    "        self.idx = 0\n",
    "        self.path_idx = 0\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "    def push(self, obs, act, rew, val, logp):\n",
    "        assert self.idx < self.capacity\n",
    "        self.obs_buf[self.idx] = obs\n",
    "        self.act_buf[self.idx] = act\n",
    "        self.rew_buf[self.idx] = rew\n",
    "        self.val_buf[self.idx] = val\n",
    "        self.logp_buf[self.idx] = logp\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "    def GAE_cal(self, last_val):\n",
    "        path_slice = slice(self.path_idx, self.idx)\n",
    "        # to make the deltas the same dim\n",
    "        rewards = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        self.rtg_buf[path_slice] = discount_cumsum(rewards, self.gamma)[:-1]\n",
    "        self.path_idx = self.idx\n",
    "\n",
    "                \n",
    "    def sample(self, minibatch_size, device):\n",
    "        \"\"\"This method sample a list of minibatches from the memory\n",
    "\n",
    "        Args:\n",
    "            minibatch_size (int): size of minibatch, usually 2^n\n",
    "            device (object): CPU or GPU\n",
    "\n",
    "        Returns:\n",
    "            list: a list of minibatches\n",
    "        \"\"\"\n",
    "        assert self.idx == self.capacity\n",
    "        # reset the index\n",
    "        self.idx, self.path_idx = 0, 0\n",
    "        # normalise advantage\n",
    "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / (np.std(self.adv_buf) + 1e-7)\n",
    "        \n",
    "        inds = np.arange(self.capacity)\n",
    "        \n",
    "        np.random.shuffle(inds)\n",
    "        \n",
    "        data = []\n",
    "        for start in range(0, self.capacity, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            minibatch_inds = inds[start:end]\n",
    "            minibatch = dict(obs=self.obs_buf[minibatch_inds], act=self.act_buf[minibatch_inds], \\\n",
    "                             rtg=self.rtg_buf[minibatch_inds], adv=self.adv_buf[minibatch_inds], \\\n",
    "                             logp=self.logp_buf[minibatch_inds])\n",
    "            data.append({k: torch.as_tensor(v, dtype=torch.float32, device=device) for k,v in minibatch.items()})\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Net(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, num_cells, continous_action):\n",
    "        super(Actor_Net,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(n_observations, num_cells)\n",
    "        self.layer2 = nn.Linear(num_cells, num_cells)\n",
    "        self.layer3 = nn.Linear(num_cells, n_actions)\n",
    "\n",
    "        self.continous_action = continous_action\n",
    "        self.action_dim = n_actions\n",
    "        \n",
    "        if self.continous_action:\n",
    "            log_std = -0.5 * np.ones(self.action_dim, dtype=np.float32)\n",
    "            # Add it to the list of parameters\n",
    "            self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))            \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        activation1 = F.tanh(self.layer1(x))\n",
    "        activation2 = F.tanh(self.layer2(activation1))\n",
    "        activation3 = self.layer3(activation2)\n",
    "\n",
    "        return activation3\n",
    "    \n",
    "    def act(self, x):\n",
    "        if self.continous_action:\n",
    "            mu = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "        else:\n",
    "            log_probs = F.log_softmax(self.forward(x), dim=1)\n",
    "            dist = Categorical(log_probs)\n",
    "    \n",
    "        action = dist.sample()\n",
    "        if self.continous_action:\n",
    "            action_logprob = dist.log_prob(action).sum(axis=-1)\n",
    "        else:\n",
    "            action_logprob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach().cpu().numpy(), action_logprob.detach().cpu().numpy()\n",
    "    \n",
    "    def logprob_ent_from_state_acton(self, x, act):\n",
    "        if self.continous_action:\n",
    "            mu = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "            # sum term is crucial to reduce dimension, otherwise the ratio = torch.exp(logp - logp_old) will have wrong result with boardcasting\n",
    "            act_logp = dist.log_prob(act).sum(axis=-1) \n",
    "        else:\n",
    "            dist = Categorical(F.softmax(self.forward(x)))\n",
    "            act_logp = dist.log_prob(act)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return entropy, act_logp\n",
    "    \n",
    "   \n",
    "class Critic_Net(nn.Module):\n",
    "    def __init__(self, n_observations, num_cells):\n",
    "        super(Critic_Net,self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, num_cells)\n",
    "        self.layer2 = nn.Linear(num_cells, num_cells)\n",
    "        self.layer3 = nn.Linear(num_cells, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activation1 = F.tanh(self.layer1(x))\n",
    "        activation2 = F.tanh(self.layer2(activation1))\n",
    "        activation3 = self.layer3(activation2)\n",
    "\n",
    "        return activation3\n",
    "\n",
    "class Actor_Critic_net(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim, continous_action, parameters_hardshare):\n",
    "\n",
    "        super(Actor_Critic_net, self).__init__()\n",
    "\n",
    "        self.parameters_hardshare = parameters_hardshare\n",
    "        self.continous_action = continous_action\n",
    "        self.act_dim = act_dim\n",
    "        if self.continous_action:\n",
    "            log_std = -0.5 * np.ones(self.act_dim, dtype=np.float32)\n",
    "            # Add it to the list of parameters\n",
    "            self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "\n",
    "        if self.parameters_hardshare:\n",
    "            self.layer1 = nn.Linear(obs_dim, hidden_dim)\n",
    "            self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "            self.actor_head = nn.Linear(hidden_dim, act_dim)\n",
    "            self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        else:\n",
    "            self.actor = Actor_Net(obs_dim, act_dim, hidden_dim, continous_action)\n",
    "            self.critic = Critic_Net(obs_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.parameters_hardshare:\n",
    "            activation1 = F.tanh(self.layer1(x))\n",
    "            activation2 = F.tanh(self.layer2(activation1))\n",
    "            actor_logits = self.actor_head(activation2)\n",
    "            value = self.critic_head(activation2)\n",
    "        else:\n",
    "            actor_logits = self.actor.forward(x)\n",
    "            value = self.critic.forward(x)\n",
    "\n",
    "        return actor_logits, value\n",
    "\n",
    "    \n",
    "    def act(self, x):\n",
    "        if self.continous_action:\n",
    "            mu, value = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "        else:\n",
    "            actor_logits, value = self.forward(x)\n",
    "            log_probs = F.log_softmax(actor_logits, dim=1)\n",
    "            dist = Categorical(log_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        if self.continous_action:\n",
    "            action_logprob = dist.log_prob(action).sum(axis=-1)\n",
    "        else:\n",
    "            action_logprob = dist.log_prob(action)\n",
    "        \n",
    "\n",
    "        return action.detach().cpu().numpy(), action_logprob.detach().cpu().numpy(), value.detach().item()     \n",
    "\n",
    "    def logprob_ent_from_state_acton(self, x, act):\n",
    "\n",
    "        if self.continous_action:\n",
    "            mu, value = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "            # sum term is crucial to reduce dimension, otherwise the ratio = torch.exp(logp - logp_old) will have wrong result with boardcasting\n",
    "            act_logp = dist.log_prob(act).sum(axis=-1) \n",
    "        else:\n",
    "            actor_logits, value = self.forward(x)\n",
    "            dist = Categorical(F.softmax(actor_logits))\n",
    "            act_logp = dist.log_prob(act)\n",
    "        entropy = dist.entropy().sum(axis=-1)\n",
    "        \n",
    "        return entropy, act_logp, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, gamma, lamb, eps_clip, K_epochs, \\\n",
    "                 observation_space, action_space, num_cells, \\\n",
    "                 actor_lr, critic_lr, memory_size , minibatch_size,\\\n",
    "                 max_training_iter, cal_total_loss, c1, c2, \\\n",
    "                 early_stop, kl_threshold, parameters_hardshare, \\\n",
    "                 max_grad_norm , device\n",
    "                 ):\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.max_training_iter = max_training_iter\n",
    "\n",
    "        self.n_observations = observation_space\n",
    "        self.n_actions = action_space\n",
    "        self.memory_size = memory_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        self.cal_total_loss = cal_total_loss\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.early_stop = early_stop\n",
    "        self.kl_threshold = kl_threshold\n",
    "\n",
    "        self.parameters_hardshare = parameters_hardshare\n",
    "        self.episode_count = 0\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.global_step = 0\n",
    "\n",
    "\n",
    "        if isinstance(action_space, Box):\n",
    "            self.continous_action = True\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.continous_action = False\n",
    "        else:\n",
    "            raise AssertionError(f\"action space is not valid {action_space}\")\n",
    "\n",
    "\n",
    "        self.observtion_dim = observation_space.shape[0]\n",
    "\n",
    "        # self.actor = Actor_Net(self.observtion_dim, \\\n",
    "        #                        action_space.shape[0] if self.continous_action else action_space.n, \\\n",
    "        #                           num_cells, self.continous_action).to(device)\n",
    "          \n",
    "        # self.critic = Critic_Net(self.observtion_dim, num_cells).to(device)\n",
    "        self.actor_critic = Actor_Critic_net(self.observtion_dim, \\\n",
    "                               action_space.shape[0] if self.continous_action else action_space.n, \\\n",
    "                                  num_cells, self.continous_action, parameters_hardshare).to(device)\n",
    "\n",
    "        # self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        # self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        if parameters_hardshare:\n",
    "            self.actor_critic_opt = torch.optim.Adam(self.actor_critic.parameters(), lr=actor_lr)\n",
    "            \n",
    "        else:\n",
    "            self.actor_critic_opt = torch.optim.Adam([ \n",
    "                {'params': self.actor_critic.actor.parameters(), 'lr': actor_lr},\n",
    "                {'params': self.actor_critic.critic.parameters(), 'lr': critic_lr} \n",
    "            ])\n",
    "\n",
    "\n",
    "        self.memory = PPOBuffer(observation_space.shape, action_space.shape, memory_size, gamma, lamb)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # wandb.watch(self.actor, log='all', log_freq=100, idx=1)\n",
    "        # wandb.watch(self.critic, log='all', log_freq=100, idx=2)\n",
    "        wandb.watch(self.actor_critic, log='all', log_freq=1000)\n",
    "\n",
    "    def roll_out(self, env):\n",
    "        # TODO: implement multiple thread\n",
    "        # make a new environment instance\n",
    "        \n",
    "\n",
    "        # Maybe a deep copy is necessary for multi-thread processing\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        \n",
    "\n",
    "        action_shape = env.action_space.shape\n",
    "        # Run the policy for T timestep\n",
    "        for i in tnrange(self.memory_size, desc=\"roll_out\", leave=False):\n",
    "\n",
    "            obs_tensor = torch.tensor(obs, \\\n",
    "                                    dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            \n",
    "            # action, action_logprob = self.actor.act(obs_tensor)\n",
    "\n",
    "            # action = action.reshape(action_shape)\n",
    "             \n",
    "            # value = self.critic.forward(obs_tensor).item()\n",
    "            \n",
    "\n",
    "            action, action_logprob, value = self.actor_critic.act(obs_tensor)\n",
    "            \n",
    "            action = action.reshape(action_shape)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            self.global_step += 1\n",
    "\n",
    "            self.memory.push(obs, action, reward, value, action_logprob)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if truncated:\n",
    "                    # last_value = self.critic.forward(torch.tensor(next_obs, dtype=torch.float32, device=self.device)).item()\n",
    "                    _, last_value = self.actor_critic.forward(torch.tensor(next_obs, dtype=torch.float32, device=self.device))\n",
    "                    last_value = last_value.item()\n",
    "                else:\n",
    "                    last_value = 0\n",
    "\n",
    "                \n",
    "                self.memory.GAE_cal(last_value)\n",
    "          \n",
    "                obs,_ = env.reset()\n",
    "                self.episode_count += 1\n",
    "                wandb.log({'episode_reward' : ep_reward}, step=self.global_step)\n",
    "                ep_reward = 0\n",
    "\n",
    "\n",
    "\n",
    "    # def compute_loss(self, data):\n",
    "    #     observations, actions, logp_old = data['obs'], data['act'], data['logp']\n",
    "    #     advs, rtgs = data['adv'], data['rtg']\n",
    "\n",
    "    #     # Calculate the pi_theta (a_t|s_t)\n",
    "    #     entropy, logp = self.actor.logprob_ent_from_state_acton(observations, actions)\n",
    "    #     ratio = torch.exp(logp - logp_old)\n",
    "    #     # Kl approx according to http://joschu.net/blog/kl-approx.html\n",
    "    #     kl_apx = ((ratio - 1) - (logp - logp_old)).mean()\n",
    "    #     wandb.log({'KL approx': kl_apx})\n",
    "    #     clip_advs = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advs\n",
    "    #     # Torch Adam implement tation mius the gradient, to plus the gradient, we need make the loss negative\n",
    "    #     actor_loss = -(torch.min(ratio*advs, clip_advs)).mean()\n",
    "\n",
    "    #     values = self.critic(observations).squeeze()\n",
    "    #     critic_loss = nn.MSELoss()(values, rtgs)\n",
    "\n",
    "    #     entropy_loss = entropy.mean()\n",
    "\n",
    "    #     return actor_loss, critic_loss, entropy_loss, kl_apx\n",
    "\n",
    "    def compute_loss(self, data):\n",
    "        observations, actions, logp_old = data['obs'], data['act'], data['logp']\n",
    "        advs, rtgs = data['adv'], data['rtg']\n",
    "\n",
    "        # Calculate the pi_theta (a_t|s_t)\n",
    "        entropy, logp, values = self.actor_critic.logprob_ent_from_state_acton(observations, actions)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        # Kl approx according to http://joschu.net/blog/kl-approx.html\n",
    "        kl_apx = ((ratio - 1) - (logp - logp_old)).mean()\n",
    "    \n",
    "        clip_advs = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advs\n",
    "        # Torch Adam implement tation mius the gradient, to plus the gradient, we need make the loss negative\n",
    "        actor_loss = -(torch.min(ratio*advs, clip_advs)).mean()\n",
    "\n",
    "        values = values.flatten() # I used squeeze before, maybe a mistake\n",
    "\n",
    "        critic_loss = F.mse_loss(values, rtgs)\n",
    "        # critic_loss = ((values - rtgs) ** 2).mean()\n",
    "\n",
    "        entropy_loss = entropy.mean()\n",
    "\n",
    "        return actor_loss, critic_loss, entropy_loss, kl_apx        \n",
    "\n",
    "    def optimise(self):\n",
    "\n",
    "        data = self.memory.sample(self.minibatch_size, self.device)\n",
    "\n",
    "        early_stop_count = 0\n",
    "\n",
    "        entropy_loss_list = []\n",
    "        actor_loss_list = []\n",
    "        critic_loss_list = []\n",
    "        kl_approx_list = []\n",
    "        \n",
    "        # for _ in tnrange(self.K_epochs, desc=f\"epochs\", position=1, leave=False):\n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            for minibatch in data:\n",
    "            \n",
    "                actor_loss, critic_loss, entropy_loss, kl_apx = self.compute_loss(minibatch)\n",
    "\n",
    "                entropy_loss_list.append(entropy_loss.item())\n",
    "                actor_loss_list.append(actor_loss.item())\n",
    "                critic_loss_list.append(critic_loss.item())\n",
    "                kl_approx_list.append(kl_apx.item())\n",
    "\n",
    "                if self.cal_total_loss:\n",
    "                    total_loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy_loss\n",
    "\n",
    "                # If this update is too big, early stop and try next minibatch\n",
    "                if self.early_stop and kl_apx > self.kl_threshold:\n",
    "                    early_stop_count += 1\n",
    "                    continue\n",
    "\n",
    "                # self.actor_opt.zero_grad()\n",
    "                # self.critic_opt.zero_grad()\n",
    "                # if self.cal_total_loss:\n",
    "                #     wandb.log({'total_loss': total_loss})\n",
    "                #     total_loss.backward()\n",
    "                #     self.actor_opt.step()\n",
    "                #     self.critic_opt.step()\n",
    "                # else:\n",
    "                #     wandb.log({'actor_loss': actor_loss, 'critic_loss': critic_loss})\n",
    "                #     actor_loss.backward()\n",
    "                #     self.actor_opt.step()\n",
    "                #     critic_loss.backward()\n",
    "                #     self.critic_opt.step()\n",
    "                \n",
    "                self.actor_critic_opt.zero_grad()\n",
    "                if self.cal_total_loss:\n",
    "                    total_loss.backward()\n",
    "                    # Used by stable-baseline3, maybe more important for RNN\n",
    "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
    "                    self.actor_critic_opt.step()\n",
    "\n",
    "                else:\n",
    "                    actor_loss.backward()\n",
    "                    critic_loss.backward()\n",
    "                    # Used by stable-baseline3, maybe more important for RNN\n",
    "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
    "                    self.actor_critic_opt.step()\n",
    "            \n",
    "        # wandb.run.summary['early_stop_count'] = early_stop_count\n",
    "        # Logging, use the same metric as stable-baselines3 to compare performance\n",
    "        wandb.log(\n",
    "            {\n",
    "                'actor_loss': np.mean(actor_loss_list),\n",
    "                'critic_loss' : np.mean(critic_loss_list),\n",
    "                'entropy_loss' : np.mean(entropy_loss_list),\n",
    "                'KL_approx' : np.mean(kl_approx_list)\n",
    "            }, step=self.global_step\n",
    "        )    \n",
    "\n",
    "                \n",
    "    def train(self, env):\n",
    "\n",
    "        for i in tnrange(self.max_training_iter // self.memory_size):\n",
    "\n",
    "            self.roll_out(env)\n",
    "\n",
    "            self.optimise()\n",
    "\n",
    "        # save the model to the wandb run folder\n",
    "        # PATH = os.path.join(wandb.run.dir, \"actor_critic.pt\")\n",
    "        # torch.save(self.actor_critic.state_dict(), PATH)\n",
    "\n",
    "        wandb.run.summary['total_episode'] = self.episode_count\n",
    "\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    run = wandb.init(\n",
    "            # project='PPO',\n",
    "        )\n",
    "\n",
    "\n",
    "    # gamma = 0.95\n",
    "    gamma = wandb.config.gamma\n",
    "    # lamb = 0.99\n",
    "    lamb = wandb.config.lam\n",
    "    eps_clip = 0.2\n",
    "    max_training_iter = 1_000_000\n",
    "    # K_epochs = 40\n",
    "    k_epochs = wandb.config.k_epochs\n",
    "    num_cells = 64\n",
    "    # actor_lr = 4e-4 \n",
    "    actor_lr = wandb.config.actor_lr\n",
    "    critic_lr = actor_lr\n",
    "    # critic_lr = wandb.config.critic_lr\n",
    "    # memory_size = 1024\n",
    "    memory_size = wandb.config.memory_size\n",
    "    # minibatch_size = 256\n",
    "    minibatch_size = wandb.config.minibatch_size\n",
    "    \n",
    "    # c1 = 0.5\n",
    "    # c2 = 0.9\n",
    "    kl_threshold = 0.013\n",
    "    c1 = wandb.config.c1\n",
    "    c2 = wandb.config.c2\n",
    "    # kl_threshold = wandb.config.kl_threshold\n",
    "    \n",
    "    env_name = \"HalfCheetah-v4\" # CartPole-v1\n",
    "    # parameters_hardshare = False\n",
    "    # early_stop = True\n",
    "    # cal_total_loss = True\n",
    "    parameters_hardshare = wandb.config.parameters_hardshare\n",
    "    early_stop = wandb.config.early_stop\n",
    "    cal_total_loss = wandb.config.cal_total_loss\n",
    "    max_grad_norm = 0.5\n",
    "\n",
    "    seed = 0\n",
    "\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            'actor_lr' : actor_lr,\n",
    "            'critic_lr' : critic_lr,\n",
    "            'gamma' : gamma,\n",
    "            'lambda' : lamb,\n",
    "            'eps_clip' : eps_clip,\n",
    "            'max_training_iter' : max_training_iter,\n",
    "            'k_epochs' : k_epochs,\n",
    "            'hidden_cell_dim' : num_cells,\n",
    "            'memory_size' : memory_size,\n",
    "            'minibatch_size' : minibatch_size,\n",
    "            'cal_total_loss' : cal_total_loss,\n",
    "            'c1' : c1,\n",
    "            'c2' : c2,\n",
    "            'early_stop' : early_stop,\n",
    "            'env_name': env_name,\n",
    "            'kl_threshold' : kl_threshold\n",
    "\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # wandb.define_metric(\"episode_reward\", summary=\"mean\")\n",
    "    # wandb.define_metric(\"KL_approx\", summary=\"mean\")\n",
    "        \n",
    "           \n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Seeding evaluation purpose\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    my_ppo = PPO(gamma, lamb, eps_clip, k_epochs, env.observation_space, env.action_space, num_cells,\\\n",
    "                 actor_lr, critic_lr, memory_size, minibatch_size, max_training_iter, \\\n",
    "                 cal_total_loss, c1, c2, early_stop, kl_threshold, parameters_hardshare, max_grad_norm, device)\n",
    "    \n",
    "    my_ppo.train(env)\n",
    "\n",
    "    env.close()\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%wandb\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep for HalfCheetah\n",
    "#### Continous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'metric':{'goal':'maximize', 'name':'episode_reward'},\n",
    "    'parameters':\n",
    "    {\n",
    "        'actor_lr' : {'value' : 3e-4},\n",
    "        'memory_size' : {'value' : 2048},\n",
    "        'k_epochs' : {'value' : 10},\n",
    "        'gamma' : {'value' : 0.99},\n",
    "        'lam' : {'value' : 0.95}, \n",
    "        'early_stop': {'value': False},\n",
    "        'cal_total_loss' : {'value' : False},\n",
    "        'parameters_hardshare' : {'value' : False},\n",
    "        'c1' : {'value': 0.5},\n",
    "        'c2' : {'value' : 0},\n",
    "        # 'kl_threshold' : {'min': 0.01, 'max': 0.04},\n",
    "        'minibatch_size' : {'value' : 64}\n",
    "    }\n",
    "}\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"PPO_GYM\"\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='PPO-HalfCheetah_cmp')\n",
    "wandb.agent(sweep_id, function=main, count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep configuration for Pendulum\n",
    "#### Continous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'grid',\n",
    "    'metric':{'goal':'maximize', 'name':'episode_reward'},\n",
    "    'parameters':\n",
    "    {\n",
    "        'early_stop': {'value': False},\n",
    "        'cal_total_loss' : {'value' : True},\n",
    "        'parameters_hardshare' : {'value' : False},\n",
    "        'c1' : {'value': 0.5020639303776493},\n",
    "        'c2' : {'value' : 0.910077248529638},\n",
    "        # 'kl_threshold' : {'min': 0.01, 'max': 0.04},\n",
    "        'minibatch_size' : {'values' : [128, 256, 512, 1024]}\n",
    "    }\n",
    "}\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"PPO_GYM\"\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='PPO-Pendulum-2')\n",
    "wandb.agent(sweep_id, function=main, count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

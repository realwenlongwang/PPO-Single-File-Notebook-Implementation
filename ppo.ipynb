{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Single File Notebook Implementation\n",
    "---\n",
    "This implementation is a single file notebook PPO implementation. The implementation is based on the PPO implementation of [**OpenAI Spinning Up**](https://github.com/openai/spinningup/tree/master) and [**Stable-Baseline3**](https://github.com/DLR-RM/stable-baselines3). This implementation uses [*wandb*](https://wandb.ai/site) to log the performance. The performance is evaluated against **Stable-Baseline3** in Mujoco environments. The result can be view [here](https://api.wandb.ai/links/tcd-clearway/kh37ft8v)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tnrange\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wandb\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import os\n",
    "import random\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from gymnasium.wrappers import NormalizeObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    \"\"\"\n",
    "    Helper function makes sure the shape of experience is correct for the buffer\n",
    "\n",
    "    Args:\n",
    "        length (int): _description_\n",
    "        shape (tuple[int,int], optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int,int]: correct shape\n",
    "    \"\"\"\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "# TODO: This buffer cannot recompute GAE. Maybe change it in the future\n",
    "class PPOBuffer():\n",
    "    \"\"\"\n",
    "    A buffer to store the rollout experience from OpenAI spinningup\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_dim, action_dim, capacity, gamma, lam):\n",
    "        self.obs_buf = np.zeros(combined_shape(capacity, observation_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(capacity, action_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.rtg_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.capacity = capacity\n",
    "        self.idx = 0\n",
    "        self.path_idx = 0\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "\n",
    "    def push(self, obs, act, rew, val, logp):\n",
    "        assert self.idx < self.capacity\n",
    "        self.obs_buf[self.idx] = obs\n",
    "        self.act_buf[self.idx] = act\n",
    "        self.rew_buf[self.idx] = rew\n",
    "        self.val_buf[self.idx] = val\n",
    "        self.logp_buf[self.idx] = logp\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "    def GAE_cal(self, last_val):\n",
    "        \"\"\"Calculate the GAE when an episode is ended\n",
    "\n",
    "        Args:\n",
    "            last_val (int): last state value, it is zero when the episode is terminated.\n",
    "            it's v(s_{t+1}) when the state truncate at t.\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_idx, self.idx)\n",
    "        # to make the deltas the same dim\n",
    "        rewards = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        ### OpenAI spinning up implemetation comment: No ideal, big value loss when episode rewards are large\n",
    "        # self.rtg_buf[path_slice] = discount_cumsum(rewards, self.gamma)[:-1]\n",
    "\n",
    "        ### OpenAI stable_baseline3 implementation\n",
    "        ### in David Silver Lecture 4: https://www.youtube.com/watch?v=PnHCvfgC_ZA\n",
    "        ### TD(lambda) estimator, see \"Telescoping in TD(lambda)\"\n",
    "        self.rtg_buf[path_slice] = self.adv_buf[path_slice] + self.val_buf[path_slice]\n",
    "        \n",
    "        self.path_idx = self.idx\n",
    "\n",
    "                \n",
    "    def sample(self, minibatch_size, device):\n",
    "        \"\"\"This method sample a list of minibatches from the memory\n",
    "\n",
    "        Args:\n",
    "            minibatch_size (int): size of minibatch, usually 2^n\n",
    "            device (object): CPU or GPU\n",
    "\n",
    "        Returns:\n",
    "            list: a list of minibatches\n",
    "        \"\"\"\n",
    "        assert self.idx == self.capacity, f'The buffer is not full, \\\n",
    "              self.idx:{self.idx} and self.capacity:{self.capacity}'\n",
    "        # normalise advantage\n",
    "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / (np.std(self.adv_buf) + 1e-8)\n",
    "        \n",
    "        inds = np.arange(self.capacity)\n",
    "        \n",
    "        np.random.shuffle(inds)\n",
    "        \n",
    "        data = []\n",
    "        for start in range(0, self.capacity, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            minibatch_inds = inds[start:end]\n",
    "            minibatch = dict(obs=self.obs_buf[minibatch_inds], act=self.act_buf[minibatch_inds], \\\n",
    "                             rtg=self.rtg_buf[minibatch_inds], adv=self.adv_buf[minibatch_inds], \\\n",
    "                             logp=self.logp_buf[minibatch_inds])\n",
    "            data.append({k: torch.as_tensor(v, dtype=torch.float32, device=device) for k,v in minibatch.items()})\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the index\n",
    "        self.idx, self.path_idx = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2)):\n",
    "    \"\"\"Init the weights as the stable baseline3 so the performance is comparable.\n",
    "       But it is not the ideal way to initialise the weights.\n",
    "\n",
    "    Args:\n",
    "        layer (_type_): layers\n",
    "        std (_type_, optional): standard deviation. Defaults to np.sqrt(2).\n",
    "\n",
    "    Returns:\n",
    "        _type_: layers after init\n",
    "    \"\"\"\n",
    "    nn.init.orthogonal_(layer.weight, std)\n",
    "    nn.init.constant_(layer.bias, 0.0)\n",
    "    return layer\n",
    "\n",
    "class Actor_Net(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, num_cells, continous_action, log_std_init=0.0):\n",
    "        super(Actor_Net,self).__init__()\n",
    "        \n",
    "        self.layer1 = layer_init(nn.Linear(n_observations, num_cells))\n",
    "        self.layer2 = layer_init(nn.Linear(num_cells, num_cells))\n",
    "        self.layer3 = layer_init(nn.Linear(num_cells, n_actions), std=0.01)\n",
    "\n",
    "        self.continous_action = continous_action\n",
    "        self.action_dim = n_actions\n",
    "        \n",
    "        if self.continous_action:\n",
    "            log_std = log_std_init * np.ones(self.action_dim, dtype=np.float32)\n",
    "            # Add it to the list of parameters\n",
    "            self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "            #\n",
    "            ### https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/  implementation\n",
    "            # self.log_std = nn.Parameter(torch.zeros(1, self.action_dim))  \n",
    "\n",
    "            ### Stable-baseline3 implementation\n",
    "            # self.log_std = nn.Parameter(torch.ones(self.action_dim) * log_std_init, requires_grad=False)      \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        activation1 = F.tanh(self.layer1(x))\n",
    "        activation2 = F.tanh(self.layer2(activation1))\n",
    "        activation3 = self.layer3(activation2)\n",
    "\n",
    "        return activation3\n",
    "    \n",
    "    def act(self, x):\n",
    "        if self.continous_action:\n",
    "            mu = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "        else:\n",
    "            log_probs = F.log_softmax(self.forward(x), dim=1)\n",
    "            dist = Categorical(log_probs)\n",
    "    \n",
    "        action = dist.sample()\n",
    "        if self.continous_action:\n",
    "            action_logprob = dist.log_prob(action).sum(axis=-1)\n",
    "        else:\n",
    "            action_logprob = dist.log_prob(action)\n",
    "\n",
    "        return action, action_logprob\n",
    "    \n",
    "    def logprob_ent_from_state_acton(self, x, act):\n",
    "        if self.continous_action:\n",
    "            mu = self.forward(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = Normal(mu, std)\n",
    "            # sum term is crucial to reduce dimension, otherwise the ratio = torch.exp(logp - logp_old) will have wrong result with boardcasting\n",
    "            act_logp = dist.log_prob(act).sum(axis=-1) \n",
    "        else:\n",
    "            dist = Categorical(F.softmax(self.forward(x)))\n",
    "            act_logp = dist.log_prob(act)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return entropy, act_logp\n",
    "    \n",
    "   \n",
    "class Critic_Net(nn.Module):\n",
    "    def __init__(self, n_observations, num_cells):\n",
    "        super(Critic_Net,self).__init__()\n",
    "        self.layer1 = layer_init(nn.Linear(n_observations, num_cells))\n",
    "        self.layer2 = layer_init(nn.Linear(num_cells, num_cells))\n",
    "        self.layer3 = layer_init(nn.Linear(num_cells, 1), std=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activation1 = F.tanh(self.layer1(x))\n",
    "        activation2 = F.tanh(self.layer2(activation1))\n",
    "        activation3 = self.layer3(activation2)\n",
    "\n",
    "        return activation3\n",
    "\n",
    "class Actor_Critic_net(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim, continous_action, continous_observation, parameters_hardshare, log_std_init=0.0):\n",
    "\n",
    "        super(Actor_Critic_net, self).__init__()\n",
    "\n",
    "        self.parameters_hardshare = parameters_hardshare\n",
    "        self.continous_action = continous_action\n",
    "        self.continous_observation = continous_observation\n",
    "        self.action_dim = act_dim\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        if self.parameters_hardshare:\n",
    "            self.layer1 = layer_init(nn.Linear(obs_dim, hidden_dim))\n",
    "            self.layer2 = layer_init(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "            self.actor_head = layer_init(nn.Linear(hidden_dim, act_dim), std=0.01)\n",
    "            self.critic_head = layer_init(nn.Linear(hidden_dim, 1), std=1.0)\n",
    "            if self.continous_action:\n",
    "                log_std = log_std_init * np.ones(self.action_dim, dtype=np.float32)\n",
    "                # Add it to the list of parameters\n",
    "                self.log_std = torch.nn.Parameter(torch.as_tensor(log_std), requires_grad=True)\n",
    "                #\n",
    "                ### https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/  implementation\n",
    "                # self.log_std = nn.Parameter(torch.zeros(1, self.action_dim))  \n",
    "\n",
    "                ### Stable-baseline3 implementation\n",
    "                # self.log_std = nn.Parameter(torch.ones(self.act_dim) * log_std_init, requires_grad=False) \n",
    "\n",
    "        else:\n",
    "            self.actor = Actor_Net(obs_dim, act_dim, hidden_dim, continous_action)\n",
    "            self.critic = Critic_Net(obs_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.continous_observation:\n",
    "            x = F.one_hot(x.long(), num_classes=self.obs_dim).float()\n",
    "        if self.parameters_hardshare:\n",
    "            activation1 = F.tanh(self.layer1(x))\n",
    "            activation2 = F.tanh(self.layer2(activation1))\n",
    "            actor_logits = self.actor_head(activation2)\n",
    "            value = self.critic_head(activation2)\n",
    "        else:\n",
    "            actor_logits = self.actor.forward(x)\n",
    "            value = self.critic.forward(x)\n",
    "\n",
    "        return actor_logits, value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        if not self.continous_observation:\n",
    "            x = F.one_hot(x, num_classes=self.obs_dim).float()\n",
    "        return self.critic(x).item()\n",
    "\n",
    "    \n",
    "    def act(self, x):\n",
    "        \"\"\"act with a state\n",
    "\n",
    "        Args:\n",
    "            x (_type_): state from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: action according to the state\n",
    "            action_logprob: the log probability to sample the action\n",
    "            value: the state value\n",
    "        \"\"\"\n",
    "        if self.continous_action:\n",
    "            mu, value = self.forward(x)\n",
    "            log_std = self.log_std if self.parameters_hardshare else self.actor.log_std\n",
    "            std = torch.exp(log_std)\n",
    "            dist = Normal(mu, std)\n",
    "        else:\n",
    "            actor_logits, value = self.forward(x)\n",
    "            log_probs = F.log_softmax(actor_logits, dim=-1)\n",
    "            dist = Categorical(log_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        if self.continous_action:\n",
    "            action_logprob = dist.log_prob(action).sum(axis=-1)\n",
    "        else:\n",
    "            action_logprob = dist.log_prob(action)\n",
    "        \n",
    "\n",
    "        return action, action_logprob, value  \n",
    "\n",
    "    def logprob_ent_from_state_acton(self, x, action):\n",
    "        \"\"\"Return the entropy, log probability of the selected action and state value\n",
    "\n",
    "        Args:\n",
    "            x (_type_): state from the environment\n",
    "            action (_type_): action\n",
    "\n",
    "        Returns:\n",
    "            entropy: entropy from the distribution that the action is sampled from\n",
    "            action_logprob: the log probability to sample the action\n",
    "            value: the state value\n",
    "        \"\"\"\n",
    "\n",
    "        if self.continous_action:\n",
    "            mu, value = self.forward(x)\n",
    "            log_std = self.log_std if self.parameters_hardshare else self.actor.log_std\n",
    "            std = torch.exp(log_std)\n",
    "            dist = Normal(mu, std)\n",
    "            ### sum in log space is equivalent to multiplication in probability space\n",
    "            ### Pr(a_1, a_2) = Pr(a_1)*Pr(a_2) given a_1 and a_2 are independent sampled\n",
    "            action_logp = dist.log_prob(action).sum(axis=-1) \n",
    "        else:\n",
    "            actor_logits, value = self.forward(x)\n",
    "            log_probs = F.log_softmax(actor_logits, dim=-1)\n",
    "            dist = Categorical(log_probs)\n",
    "            action_logp = dist.log_prob(action)\n",
    "        entropy = dist.entropy().sum(axis=-1)\n",
    "        \n",
    "        return entropy, action_logp, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, gamma, lamb, eps_clip, K_epochs, \\\n",
    "                 observation_space, action_space, num_cells, \\\n",
    "                 actor_lr, critic_lr, memory_size , minibatch_size,\\\n",
    "                 max_training_iter, cal_total_loss, c1, c2, \\\n",
    "                 early_stop, kl_threshold, parameters_hardshare, \\\n",
    "                 max_grad_norm , device\n",
    "                 ):\n",
    "        \"\"\"Init\n",
    "\n",
    "        Args:\n",
    "            gamma (float): discount factor of future value\n",
    "            lamb (float): lambda factor from GAE from 0 to 1\n",
    "            eps_clip (float): clip range, usually 0.2\n",
    "            K_epochs (in): how many times learn from one batch\n",
    "            action_space (tuple[int, int]): action space of environment\n",
    "            num_cells (int): how many cells per hidden layer\n",
    "            critic_lr (float): learning rate of the critic\n",
    "            memory_size (int): the size of rollout buffer\n",
    "            minibatch_size (int): minibatch size\n",
    "            cal_total_loss (bool): add entropy loss to the actor loss or not\n",
    "            c1 (float): coefficient for value loss\n",
    "            c2 (float): coefficient for entropy loss\n",
    "            kl_threshold (float): approx kl divergence, use for early stop\n",
    "            parameters_hardshare (bool): whether to share the first two layers of actor and critic\n",
    "            device (_type_): tf device\n",
    "\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.max_training_iter = max_training_iter\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.memory_size = memory_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        self.cal_total_loss = cal_total_loss\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.early_stop = early_stop\n",
    "        self.kl_threshold = kl_threshold\n",
    "\n",
    "        self.parameters_hardshare = parameters_hardshare\n",
    "        self.episode_count = 1\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.global_step = 0\n",
    "\n",
    "        self._last_obs = None\n",
    "        self._episode_reward = 0\n",
    "        self._early_stop_count = 0\n",
    "\n",
    "        if isinstance(action_space, Box):\n",
    "            self.continous_action = True\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.continous_action = False\n",
    "        else:\n",
    "            raise AssertionError(f\"action space is not valid {action_space}\")\n",
    "\n",
    "        if isinstance(observation_space, Box):\n",
    "            self.continous_observation = True\n",
    "        elif isinstance(observation_space, Discrete):\n",
    "            self.continous_observation = False\n",
    "        else:\n",
    "            raise AssertionError(f\"observation space is not valid {observation_space}\")        \n",
    "\n",
    "        self.actor_critic = Actor_Critic_net(\n",
    "            observation_space.shape[0] if self.continous_observation else observation_space.n, \n",
    "            action_space.shape[0] if self.continous_action else action_space.n, \n",
    "            num_cells, self.continous_action, self.continous_observation, parameters_hardshare).to(device)\n",
    "\n",
    "        if parameters_hardshare:\n",
    "            ### eps=1e-5 follows stable-baseline3\n",
    "            self.actor_critic_opt = torch.optim.Adam(self.actor_critic.parameters(), lr=actor_lr, eps=1e-5)\n",
    "            \n",
    "        else:\n",
    "            self.actor_critic_opt = torch.optim.Adam([ \n",
    "                {'params': self.actor_critic.actor.parameters(), 'lr': actor_lr, 'eps' : 1e-5},\n",
    "                {'params': self.actor_critic.critic.parameters(), 'lr': critic_lr, 'eps' : 1e-5} \n",
    "            ])\n",
    "\n",
    "\n",
    "        self.memory = PPOBuffer(observation_space.shape, action_space.shape, memory_size, gamma, lamb)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # These two lines monitor the weights and gradients\n",
    "        wandb.watch(self.actor_critic.actor, log='all', log_freq=1000, idx=1)\n",
    "        wandb.watch(self.actor_critic.critic, log='all', log_freq=1000, idx=2)\n",
    "        # wandb.watch(self.actor_critic, log='all', log_freq=1000)\n",
    "\n",
    "    def roll_out(self, env):\n",
    "        \"\"\"rollout for experience\n",
    "\n",
    "        Args:\n",
    "            env (gymnasium.Env): environment from gymnasium\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        assert self._last_obs is not None, \"No previous observation\"\n",
    "        \n",
    "        action_shape = env.action_space.shape\n",
    "        # Run the policy for T timestep\n",
    "        for i in range(self.memory_size):\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.tensor(self._last_obs, \\\n",
    "                                        dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            \n",
    "                action, action_logprob, value = self.actor_critic.act(obs_tensor)\n",
    "            if self.continous_action:\n",
    "                action = action.cpu().numpy().reshape(action_shape)\n",
    "            else:\n",
    "                action = action.item()\n",
    "\n",
    "            action_logprob = action_logprob.item()\n",
    "\n",
    "            value = value.item()\n",
    "\n",
    "            ### Clipping actions when they are reals is important\n",
    "            clipped_action = action\n",
    "\n",
    "            if self.continous_action:\n",
    "                clipped_action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(clipped_action)\n",
    "\n",
    "            self.global_step += 1\n",
    "\n",
    "            self.memory.push(self._last_obs, action, reward, value, action_logprob)\n",
    "\n",
    "            self._last_obs = next_obs\n",
    "\n",
    "            self._episode_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if truncated:\n",
    "                    with torch.no_grad():\n",
    "                        input_tensor = torch.tensor(next_obs, dtype=torch.float32, device=self.device) if self.continous_observation else torch.tensor(next_obs, dtype=torch.long, device=self.device)\n",
    "                        last_value = self.actor_critic.get_value(input_tensor)\n",
    "                else:\n",
    "                    last_value = 0\n",
    "                \n",
    "                self.memory.GAE_cal(last_value)\n",
    "\n",
    "                self._last_obs, _ = env.reset()\n",
    "                \n",
    "                self.episode_count += 1\n",
    "\n",
    "                wandb.log({'episode_reward' : self._episode_reward}, step=self.global_step)\n",
    "\n",
    "                self._episode_reward = 0\n",
    "\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(next_obs, dtype=torch.float32, device=self.device) if self.continous_observation else torch.tensor(next_obs, dtype=torch.long, device=self.device)\n",
    "            last_value = self.actor_critic.get_value(input_tensor)\n",
    "        self.memory.GAE_cal(last_value)\n",
    "\n",
    "\n",
    "    def evaluate_recording(self, env):\n",
    "        self.actor_critic.eval()\n",
    "        \n",
    "        env_name = env.spec.id\n",
    "\n",
    "        video_folder = os.path.join(wandb.run.dir, 'videos')\n",
    "\n",
    "        env = RecordVideo(env, video_folder, name_prefix=env_name)\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        action_shape = env.action_space.shape\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, \\\n",
    "                                    dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = self.actor_critic.act(obs_tensor)\n",
    "\n",
    "            if self.continous_action:\n",
    "                action = action.cpu().numpy().reshape(action_shape)\n",
    "            else:\n",
    "                action = action.item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "\n",
    "        \n",
    "        mp4_files = [file for file in os.listdir(video_folder) if file.endswith(\".mp4\")]\n",
    "\n",
    "        for mp4_file in mp4_files:\n",
    "            wandb.log({'Episode_recording': wandb.Video(os.path.join(video_folder, mp4_file))})\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "\n",
    "\n",
    "    def compute_loss(self, data):\n",
    "        \"\"\"compute the loss of state value, policy and entropy\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): minibatch with experience\n",
    "\n",
    "        Returns:\n",
    "            actor_loss : policy loss\n",
    "            critic_loss : value loss\n",
    "            entropy_loss : mean entropy of action distribution\n",
    "        \"\"\"\n",
    "        observations, actions, logp_old = data['obs'], data['act'], data['logp']\n",
    "        advs, rtgs = data['adv'], data['rtg']\n",
    "\n",
    "        # Calculate the pi_theta (a_t|s_t)\n",
    "        entropy, logp, values = self.actor_critic.logprob_ent_from_state_acton(observations, actions)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        # Kl approx according to http://joschu.net/blog/kl-approx.html\n",
    "        kl_apx = ((ratio - 1) - (logp - logp_old)).mean()\n",
    "    \n",
    "        clip_advs = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advs\n",
    "        # Torch Adam implement tation mius the gradient, to plus the gradient, we need make the loss negative\n",
    "        actor_loss = -(torch.min(ratio*advs, clip_advs)).mean()\n",
    "\n",
    "        values = values.flatten() # I used squeeze before, maybe a mistake\n",
    "\n",
    "        critic_loss = F.mse_loss(values, rtgs)\n",
    "        # critic_loss = ((values - rtgs) ** 2).mean()\n",
    "\n",
    "        entropy_loss = entropy.mean()\n",
    "\n",
    "        return actor_loss, critic_loss, entropy_loss, kl_apx        \n",
    "\n",
    "    def optimise(self):\n",
    "\n",
    "        entropy_loss_list = []\n",
    "        actor_loss_list = []\n",
    "        critic_loss_list = []\n",
    "        kl_approx_list = []\n",
    "        \n",
    "        # for _ in tnrange(self.K_epochs, desc=f\"epochs\", position=1, leave=False):\n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            # resample the minibatch every epochs\n",
    "            data = self.memory.sample(self.minibatch_size, self.device)\n",
    "            \n",
    "            for minibatch in data:\n",
    "            \n",
    "                actor_loss, critic_loss, entropy_loss, kl_apx = self.compute_loss(minibatch)\n",
    "\n",
    "                entropy_loss_list.append(-entropy_loss.item())\n",
    "                actor_loss_list.append(actor_loss.item())\n",
    "                critic_loss_list.append(critic_loss.item())\n",
    "                kl_approx_list.append(kl_apx.item())\n",
    "\n",
    "                if self.cal_total_loss:\n",
    "                    total_loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy_loss\n",
    "\n",
    "                ### If this update is too big, early stop and try next minibatch\n",
    "                if self.early_stop and kl_apx > self.kl_threshold:\n",
    "                    self._early_stop_count += 1\n",
    "                    ### OpenAI spinning up uses break as they use fullbatch instead of minibatch\n",
    "                    ### Stable-baseline3 uses break, which is questionable as they drop the rest\n",
    "                    ### of minibatches.\n",
    "                    continue\n",
    "                \n",
    "                self.actor_critic_opt.zero_grad()\n",
    "                if self.cal_total_loss:\n",
    "                    total_loss.backward()\n",
    "                    # Used by stable-baseline3, maybe more important for RNN\n",
    "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
    "                    self.actor_critic_opt.step()\n",
    "\n",
    "                else:\n",
    "                    actor_loss.backward()\n",
    "                    critic_loss.backward()\n",
    "                    # Used by stable-baseline3, maybe more important for RNN\n",
    "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
    "                    self.actor_critic_opt.step()\n",
    "\n",
    "        self.memory.reset()    \n",
    "        # Logging, use the same metric as stable-baselines3 to compare performance\n",
    "        with torch.no_grad():\n",
    "            if self.continous_action:\n",
    "                mean_std = np.exp(self.actor_critic.actor.log_std.mean().item())\n",
    "                wandb.log({'mean_std': mean_std})\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'actor_loss': np.mean(actor_loss_list),\n",
    "                'critic_loss' : np.mean(critic_loss_list),\n",
    "                'entropy_loss' : np.mean(entropy_loss_list),\n",
    "                'KL_approx' : np.mean(kl_approx_list)\n",
    "            }, step=self.global_step\n",
    "        )\n",
    "        if self.early_stop:\n",
    "            wandb.run.summary['early_stop_count'] = self._early_stop_count \n",
    "\n",
    "                \n",
    "    def train(self, env):\n",
    "        self.actor_critic.train()\n",
    "\n",
    "        self._last_obs, _ = env.reset()\n",
    "\n",
    "        for i in tnrange(self.max_training_iter // self.memory_size):\n",
    "\n",
    "            self.roll_out(env)\n",
    "\n",
    "            self.optimise()\n",
    "\n",
    "        # save the model to the wandb run folder\n",
    "        # PATH = os.path.join(wandb.run.dir, \"actor_critic.pt\")\n",
    "        # torch.save(self.actor_critic.state_dict(), PATH)\n",
    "\n",
    "\n",
    "        wandb.run.summary['total_episode'] = self.episode_count\n",
    "\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep Configuration\n",
    "### Mujoco environment\n",
    "---\n",
    "This sweep evaluates the performance of my PPO with Mujoco environments. The hyperparameters follow the original PPO paper. Please check [this](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration) for more detail about *wandb* sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'episode_reward'},\n",
    "    'parameters': {\n",
    "        'actor_lr': {'values': [1e-4, 3e-4, 1e-3]},\n",
    "        'critic_lr': {'values': [1e-4, 3e-4, 1e-3]},\n",
    "        'memory_size': {'values': [1024, 2048, 4096]},\n",
    "        'k_epochs': {'values': [5, 10, 20]},\n",
    "        'gamma': {'values': [0.95, 0.99]},\n",
    "        'lam': {'values': [0.90, 0.95]},\n",
    "        'early_stop': {'value': False},\n",
    "        'cal_total_loss': {'value': True},\n",
    "        'parameters_hardshare': {'value': False},\n",
    "        'seed': {'value': 43201},\n",
    "        'c1': {'values': [0.1, 0.5, 1.0]},\n",
    "        'c2': {'values': [0, 0.01, 0.1]},\n",
    "        'minibatch_size': {'values': [32, 64, 128]},\n",
    "        'kl_threshold': {'value': 0.15},\n",
    "        'max_grad_norm': {'values': [0.5, 1.0]},\n",
    "        'eps_clip': {'values': [0.1, 0.2, 0.3]},\n",
    "        'hidden_dim': {'values': [64, 128, 256]},\n",
    "        'max_iterations': {'distribution': 'int_uniform', 'min': 50_000, 'max': 300_000},\n",
    "        # 'max_iterations': {'value': 50000},\n",
    "        'env_name': {'value': 'Taxi-v3'},\n",
    "    },\n",
    "\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 20,\n",
    "        'max_iter': 100,\n",
    "        's': 2\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(debug=False, env_name='Walker2d-v4'):\n",
    "\n",
    "\n",
    "\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        run = wandb.init(\n",
    "            project='PPO-test',\n",
    "            mode='disabled',\n",
    "            # config = sweep_configuration\n",
    "        )\n",
    "        gamma = 0.99\n",
    "        lamb = 0.95\n",
    "        eps_clip = 0.2\n",
    "        max_training_iter = 1_000_000\n",
    "        k_epochs = 10\n",
    "        num_cells = 64\n",
    "        actor_lr = 3e-4 \n",
    "        critic_lr = actor_lr\n",
    "        memory_size = 2048\n",
    "        minibatch_size = 64    \n",
    "        c1 = 0.5\n",
    "        c2 = 0\n",
    "        kl_threshold = 0.15\n",
    "        env_name = env_name\n",
    "        parameters_hardshare = False\n",
    "        early_stop = False\n",
    "        cal_total_loss = False\n",
    "        max_grad_norm = 0.5\n",
    "        seed = 123456\n",
    "\n",
    "        wandb.config.update(\n",
    "            {\n",
    "                'actor_lr' : actor_lr,\n",
    "                'critic_lr' : critic_lr,\n",
    "                'gamma' : gamma,\n",
    "                'lambda' : lamb,\n",
    "                'eps_clip' : eps_clip,\n",
    "                'max_training_iter' : max_training_iter,\n",
    "                'k_epochs' : k_epochs,\n",
    "                'hidden_cell_dim' : num_cells,\n",
    "                'memory_size' : memory_size,\n",
    "                'minibatch_size' : minibatch_size,\n",
    "                'c1' : c1,\n",
    "                'c2' : c2,\n",
    "                'kl_threshold' : kl_threshold,\n",
    "                'env_name': env_name,\n",
    "                'early_stop' : early_stop,\n",
    "                'parameters_hardshare' : parameters_hardshare,\n",
    "                'early_stop' : early_stop,\n",
    "                'cal_total_loss' : cal_total_loss,\n",
    "                'max_grad_norm' : max_grad_norm,\n",
    "                'seed' : seed\n",
    "            },  allow_val_change=True\n",
    "        )   \n",
    "    else:\n",
    "        run = wandb.init()\n",
    "        gamma = wandb.config.gamma\n",
    "        lamb = wandb.config.lam\n",
    "        k_epochs = wandb.config.k_epochs\n",
    "        actor_lr = wandb.config.actor_lr\n",
    "        critic_lr = wandb.config.critic_lr\n",
    "        memory_size = wandb.config.memory_size\n",
    "        minibatch_size = wandb.config.minibatch_size\n",
    "        c1 = wandb.config.c1\n",
    "        c2 = wandb.config.c2\n",
    "        kl_threshold = wandb.config.kl_threshold\n",
    "        env_name = wandb.config.env_name\n",
    "        parameters_hardshare = wandb.config.parameters_hardshare\n",
    "        early_stop = wandb.config.early_stop\n",
    "        cal_total_loss = wandb.config.cal_total_loss\n",
    "        max_grad_norm = wandb.config.max_grad_norm\n",
    "        seed = wandb.config.seed\n",
    "        eps_clip = wandb.config.eps_clip\n",
    "        num_cells = wandb.config.hidden_dim\n",
    "        max_training_iter = wandb.config.max_iterations\n",
    "        seed = wandb.config.seed  \n",
    "\n",
    "\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            'implementation': 'my_ppo'\n",
    "        }\n",
    "    )\n",
    "    # wandb.define_metric(\"episode_reward\", summary=\"mean\")\n",
    "    # wandb.define_metric(\"KL_approx\", summary=\"mean\")\n",
    "        \n",
    "    # Using render_mode slow the training process down\n",
    "    env = gym.make(env_name)\n",
    "    recording_env = gym.make(env_name, render_mode = 'rgb_array_list')\n",
    "\n",
    "    # Seeding for evaluation purpose\n",
    "    env.np_random = np.random.default_rng(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "\n",
    "    recording_env.np_random = np.random.default_rng(seed)\n",
    "    recording_env.action_space.seed(seed)\n",
    "    recording_env.observation_space.seed(seed)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Deterministic operations for CuDNN, it may impact performances\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    my_ppo = PPO(gamma, lamb, eps_clip, k_epochs, env.observation_space, env.action_space, num_cells,\\\n",
    "                 actor_lr, critic_lr, memory_size, minibatch_size, max_training_iter, \\\n",
    "                 cal_total_loss, c1, c2, early_stop, kl_threshold, parameters_hardshare, max_grad_norm, device)\n",
    "    \n",
    "    my_ppo.train(env)\n",
    "    my_ppo.evaluate_recording(recording_env)\n",
    "\n",
    "    env.close()\n",
    "    recording_env.close()\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(debug=True, env_name='Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %env \"WANDB_NOTEBOOK_NAME\" \"PPO_GYM\"\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='PPO_taxi_sweep')\n",
    "wandb.agent(sweep_id, function=main, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
